{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8742fd88",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ./tf_retinaface_mbv2/\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# MTCNN face detector\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#from mtcnn.mtcnn import MTCNN\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#detector = MTCNN()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# RetinaFace face detector\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m detector_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tf_retinaface_mbv2/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_face\u001b[39m(frame, bbs, pointss):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# process only one face (center ?)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     offsets \u001b[38;5;241m=\u001b[39m [(bbs[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mbbs[:,\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39mframe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     15\u001b[0m                (bbs[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mbbs[:,\u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39mframe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\R2\\python\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:936\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model.load\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model.load_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(export_dir, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    847\u001b[0m   \u001b[38;5;124;03m\"\"\"Load a SavedModel from `export_dir`.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03m  Signatures associated with the SavedModel are available as functions:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    ValueError: If `tags` don't match a MetaGraph in the SavedModel.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 936\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mload_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Desktop\\R2\\python\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:949\u001b[0m, in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, \u001b[38;5;28mset\u001b[39m):\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m   tags \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(tags)\n\u001b[0;32m    948\u001b[0m saved_model_proto, debug_info \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 949\u001b[0m     \u001b[43mloader_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_saved_model_with_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_graph_def\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    953\u001b[0m   metrics\u001b[38;5;241m.\u001b[39mIncrementReadApi(_LOAD_V2_LABEL)\n",
      "File \u001b[1;32m~\\Desktop\\R2\\python\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:57\u001b[0m, in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_saved_model_with_debug_info\u001b[39m(export_dir):\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;124;03m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m   saved_model \u001b[38;5;241m=\u001b[39m \u001b[43mparse_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m   debug_info_path \u001b[38;5;241m=\u001b[39m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     60\u001b[0m       saved_model_utils\u001b[38;5;241m.\u001b[39mget_debug_dir(export_dir),\n\u001b[0;32m     61\u001b[0m       constants\u001b[38;5;241m.\u001b[39mDEBUG_INFO_FILENAME_PB)\n\u001b[0;32m     62\u001b[0m   debug_info \u001b[38;5;241m=\u001b[39m graph_debug_info_pb2\u001b[38;5;241m.\u001b[39mGraphDebugInfo()\n",
      "File \u001b[1;32m~\\Desktop\\R2\\python\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:115\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot parse file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_pbtxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel file does not exist at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: ./tf_retinaface_mbv2/\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# MTCNN face detector\n",
    "#from mtcnn.mtcnn import MTCNN\n",
    "#detector = MTCNN()\n",
    "\n",
    "# RetinaFace face detector\n",
    "detector_model = tf.saved_model.load('./tf_retinaface_mbv2/')\n",
    "\n",
    "def one_face(frame, bbs, pointss):\n",
    "    # process only one face (center ?)\n",
    "    offsets = [(bbs[:,0]+bbs[:,2])/2-frame.shape[1]/2,\n",
    "               (bbs[:,1]+bbs[:,3])/2-frame.shape[0]/2]\n",
    "    offset_dist = np.sum(np.abs(offsets),0)\n",
    "    index = np.argmin(offset_dist)\n",
    "    bb = bbs[index]\n",
    "    points = pointss[:,index]\n",
    "    return bb, points\n",
    "            \n",
    "def draw_landmarks(frame, bb, points):\n",
    "    # draw rectangle and landmarks on face\n",
    "    cv2.rectangle(frame,(int(bb[0]),int(bb[1])),(int(bb[2]),int(bb[3])),orange,2)\n",
    "    cv2.circle(frame, (int(points[0]), int(points[5])), 2, (255,0,0), 2)# eye\n",
    "    cv2.circle(frame, (int(points[1]), int(points[6])), 2, (255,0,0), 2)\n",
    "    cv2.circle(frame, (int(points[2]), int(points[7])), 2, (255,0,0), 2)# nose\n",
    "    cv2.circle(frame, (int(points[3]), int(points[8])), 2, (255,0,0), 2)# mouth\n",
    "    cv2.circle(frame, (int(points[4]), int(points[9])), 2, (255,0,0), 2)\n",
    "    \n",
    "    w = int(bb[2])-int(bb[0])# width\n",
    "    h = int(bb[3])-int(bb[1])# height\n",
    "    w2h_ratio = w/h# ratio\n",
    "    eye2box_ratio = (points[0]-bb[0]) / (bb[2]-points[1])\n",
    "    \n",
    "    cv2.putText(frame, \"Width (pixels): {}\".format(w), (10,30), font, font_size, red, 1)\n",
    "    cv2.putText(frame, \"Height (pixels): {}\".format(h), (10,40), font, font_size, red, 1)\n",
    "    \n",
    "    if w2h_ratio < 0.7 or w2h_ratio > 0.9:\n",
    "        #cv2.putText(frame, \"width/height: {0:.2f}\".format(w2h_ratio), (10,40), font, font_size, blue, 1)\n",
    "        cv2.putText(frame, \"Narrow Face\", (10,60), font, font_size, red, 1)\n",
    "    if eye2box_ratio > 1.5 or eye2box_ratio < 0.88:\n",
    "        #cv2.putText(frame, \"leye2lbox/reye2rbox: {0:.2f}\".format((points[0]-bb[0]) / (bb[2]-points[1])), (10,70), font, font_size, red, 1)\n",
    "        cv2.putText(frame, \"Acentric Face\", (10,70), font, font_size, red, 1)\n",
    "\n",
    "def find_smile(pts):\n",
    "    dx_eyes = pts[1] - pts[0]# between pupils\n",
    "    dx_mout = pts[4] - pts[3]# between mouth corners\n",
    "    smile_ratio = dx_mout/dx_eyes    \n",
    "    return smile_ratio\n",
    "\n",
    "def find_roll(pts):\n",
    "    return pts[6] - pts[5]\n",
    "\n",
    "def find_yaw(pts):\n",
    "    le2n = pts[2] - pts[0]\n",
    "    re2n = pts[1] - pts[2]\n",
    "    return le2n - re2n\n",
    "\n",
    "def find_pitch(pts):\n",
    "    eye_y = (pts[5] + pts[6]) / 2\n",
    "    mou_y = (pts[8] + pts[9]) / 2\n",
    "    e2n = eye_y - pts[7]\n",
    "    n2m = pts[7] - mou_y\n",
    "    return e2n/n2m\n",
    "\n",
    "def find_pose(points):\n",
    "    X = points[0:5]\n",
    "    Y = points[5:10]\n",
    "\n",
    "    angle = np.arctan((Y[1]-Y[0])/(X[1]-X[0]))/np.pi*180\n",
    "    alpha = np.cos(np.deg2rad(angle))\n",
    "    beta = np.sin(np.deg2rad(angle))\n",
    "    \n",
    "    # rotated points\n",
    "    Xr = np.zeros((5))\n",
    "    Yr = np.zeros((5))\n",
    "    for i in range(5):\n",
    "        Xr[i] = alpha*X[i]+beta*Y[i]+(1-alpha)*X[2]-beta*Y[2]\n",
    "        Yr[i] = -beta*X[i]+alpha*Y[i]+beta*X[2]+(1-alpha)*Y[2]\n",
    "\n",
    "    # average distance between eyes and mouth\n",
    "    dXtot = (Xr[1]-Xr[0]+Xr[4]-Xr[3])/2\n",
    "    dYtot = (Yr[3]-Yr[0]+Yr[4]-Yr[1])/2\n",
    "\n",
    "    # average distance between nose and eyes\n",
    "    dXnose = (Xr[1]-Xr[2]+Xr[4]-Xr[2])/2\n",
    "    dYnose = (Yr[3]-Yr[2]+Yr[4]-Yr[2])/2\n",
    "\n",
    "    # relative rotation 0% is frontal 100% is profile\n",
    "    Xfrontal = np.abs(np.clip(-90+90/0.5*dXnose/dXtot,-90,90))\n",
    "    Yfrontal = np.abs(np.clip(-90+90/0.5*dYnose/dYtot,-90,90))\n",
    "\n",
    "    return Xfrontal, Yfrontal# horizontal and vertical angles\n",
    "\n",
    "def face_detector(image, image_shape_max=640, score_min=None, pixel_min=None, pixel_max=None, Ain_min=None):\n",
    "    '''\n",
    "    Performs face detection using retinaface method with speed boost and initial quality checks based on whole image size\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : uint8\n",
    "        image for face detection.\n",
    "    image_shape_max : int, optional\n",
    "        maximum size (in pixels) of image. The default is None.\n",
    "    score_min : float, optional\n",
    "        minimum detection score (0 to 1). The default is None.\n",
    "    pixel_min : int, optional\n",
    "        mininmum face size based on heigth of bounding box. The default is None.\n",
    "    pixel_max : int, optional\n",
    "        maximum face size based on heigth of bounding box. The default is None.\n",
    "    Ain_min : float, optional\n",
    "        minimum area of face in bounding box. The default is None.\n",
    "    Returns\n",
    "    -------\n",
    "    float array\n",
    "        landmarks.\n",
    "    float array\n",
    "        bounding boxes.\n",
    "    flaot array\n",
    "        detection scores.\n",
    "    float array\n",
    "        face area in bounding box.\n",
    "    '''\n",
    "\n",
    "    image_shape = image.shape[:2]\n",
    "    \n",
    "    # perform image resize for faster detection    \n",
    "    if image_shape_max:\n",
    "        scale_factor = max([1, max(image_shape)/image_shape_max])\n",
    "    else:\n",
    "        scale_factor = 1\n",
    "        \n",
    "    if scale_factor > 1:        \n",
    "        scaled_image = cv2.resize(image, (0, 0), fx=1/scale_factor, fy=1/scale_factor)\n",
    "        bbs_all, points_all = retinaface(scaled_image)\n",
    "        bbs_all[:,:4]*=scale_factor\n",
    "        points_all*=scale_factor\n",
    "    else:\n",
    "        bbs_all, points_all = retinaface(image)\n",
    "    \n",
    "    bbs=bbs_all.copy()\n",
    "    points=points_all.copy()\n",
    "    \n",
    "    # check detection score\n",
    "    if score_min:\n",
    "        mask=np.array(bbs[:,4]>score_min)\n",
    "        bbs=bbs[mask]\n",
    "        points=points[mask]\n",
    "        if len(bbs)==0:\n",
    "            return [],[],[],[]           \n",
    "\n",
    "    # check pixel height\n",
    "    if pixel_min: \n",
    "        pixel=bbs[:,3]-bbs[:,1]\n",
    "        mask=np.array(pixel>pixel_min)\n",
    "        bbs=bbs[mask]\n",
    "        points=points[mask]\n",
    "        if len(bbs)==0:\n",
    "            return [],[],[],[]           \n",
    "\n",
    "    if pixel_max: \n",
    "        pixel=bbs[:,3]-bbs[:,1]\n",
    "        mask=np.array(pixel<pixel_max)\n",
    "        bbs=bbs[mask]\n",
    "        points=points[mask]\n",
    "        if len(bbs)==0:\n",
    "            return [],[],[],[]           \n",
    "\n",
    "    # check face area in bounding box\n",
    "    Ains = []\n",
    "    for bb in bbs:\n",
    "        Win=min(image_shape[1],bb[2])-max(0,bb[0])\n",
    "        Hin=min(image_shape[0],bb[3])-max(0,bb[1])\n",
    "        Abb=(bb[2]-bb[0])*(bb[3]-bb[1])\n",
    "        Ains.append(Win*Hin/Abb*100 if Abb!=0 else 0)\n",
    "    Ains = np.array(Ains)\n",
    "\n",
    "    if Ain_min:\n",
    "        mask=np.array(Ains>=Ain_min)\n",
    "        bbs=bbs[mask]\n",
    "        points=points[mask]\n",
    "        Ains=Ains[mask]\n",
    "        if len(bbs)==0:\n",
    "            return [],[],[],[]           \n",
    "    \n",
    "    scores = bbs[:,-1]\n",
    "    bbs = bbs[:, :4]\n",
    "    \n",
    "    return points, bbs, scores, Ains\n",
    "\n",
    "def retinaface(image):\n",
    "\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    \n",
    "    image_pad, pad_params = pad_input_image(image)    \n",
    "    image_pad = tf.convert_to_tensor(image_pad[np.newaxis, ...])\n",
    "    image_pad = tf.cast(image_pad, tf.float32)  \n",
    "   \n",
    "    outputs = detector_model(image_pad).numpy()\n",
    "\n",
    "    outputs = recover_pad_output(outputs, pad_params)\n",
    "    Nfaces = len(outputs)\n",
    "    \n",
    "    bbs = np.zeros((Nfaces,5))\n",
    "    lms = np.zeros((Nfaces,10))\n",
    "    \n",
    "    bbs[:,[0,2]] = outputs[:,[0,2]]*width\n",
    "    bbs[:,[1,3]] = outputs[:,[1,3]]*height\n",
    "    bbs[:,4] = outputs[:,-1]\n",
    "    \n",
    "    lms[:,0:5] = outputs[:,[4,6,8,10,12]]*width\n",
    "    lms[:,5:10] = outputs[:,[5,7,9,11,13]]*height\n",
    "    \n",
    "    return bbs, lms\n",
    "\n",
    "def pad_input_image(img, max_steps=32):\n",
    "    \"\"\"pad image to suitable shape\"\"\"\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    img_pad_h = 0\n",
    "    if img_h % max_steps > 0:\n",
    "        img_pad_h = max_steps - img_h % max_steps\n",
    "\n",
    "    img_pad_w = 0\n",
    "    if img_w % max_steps > 0:\n",
    "        img_pad_w = max_steps - img_w % max_steps\n",
    "\n",
    "    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n",
    "    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n",
    "                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n",
    "    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n",
    "\n",
    "    return img, pad_params\n",
    "\n",
    "def recover_pad_output(outputs, pad_params):\n",
    "    \"\"\"recover the padded output effect\"\"\"\n",
    "    img_h, img_w, img_pad_h, img_pad_w = pad_params\n",
    "    recover_xy = np.reshape(outputs[:, :14], [-1, 7, 2]) * \\\n",
    "        [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n",
    "    outputs[:, :14] = np.reshape(recover_xy, [-1, 14])\n",
    "\n",
    "    return outputs\n",
    "\n",
    "#============================================================================\n",
    "# FONT SETTING (font style, size and color)\n",
    "font = cv2.FONT_HERSHEY_COMPLEX # Text in video\n",
    "font_size = 0.4\n",
    "blue = (225,0,0)\n",
    "green = (0,128,0)\n",
    "red = (0,0,255)\n",
    "orange = (0,140,255)\n",
    "\n",
    "# DEMO GUI SETTING\n",
    "total_size = np.array([750, 1400], dtype=int) # demo-gui size (resolution)\n",
    "# complete/final frame to be shown on demo-gui\n",
    "frame_show = np.ones((total_size[0], total_size[1], 3), dtype='uint8')*255 \n",
    "logo_size = 150\n",
    "show_size = 150 # Size showed detected faces\n",
    "res_max = np.zeros((2), dtype=int)\n",
    "res_resize = np.zeros((2), dtype=int)\n",
    "\n",
    "\n",
    "# RECORDING SETTING (Recordings on/off)\n",
    "image_save = False# save face image\n",
    "video_save = False# save video\n",
    "fps = 10.\n",
    "video_format = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "if video_save:\n",
    "    video_file = 'video_out.avi'\n",
    "    video_out = cv2.VideoWriter(video_file, video_format, fps, (640,480))\n",
    "\n",
    "# CAMERA SETTING (video capture initialization)\n",
    "camera = 0#0: internal, 1: external\n",
    "cap = cv2.VideoCapture(camera)\n",
    "\n",
    "res_actual = np.zeros((1,2), dtype=int)# actual resolution of the camera\n",
    "res_actual[0,0] = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "res_actual[0,1] = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "print(\"\\ncamera resolution: {}\".format(res_actual))\n",
    "\n",
    "\n",
    "# PROCESS FRAMES\n",
    "while (True): \n",
    "    \n",
    "    ret, frame = cap.read()# read frame from camera\n",
    "    if not (ret):\n",
    "        break\n",
    "    \n",
    "    frame = np.array(frame)\n",
    "    frame = cv2.flip(frame,1)\n",
    "       \n",
    "    res_crop = np.asarray(frame.shape)[0:2]# ?   \n",
    "    \n",
    "    #bbs_all, pointss_all = detector.detect_faces(frame)# face detection\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pointss_all, bbs_all, scores_all, _ = face_detector(frame_rgb, image_shape_max=640, score_min=0.95,\n",
    "                                                      pixel_min=20, pixel_max=1000, Ain_min=90)\n",
    "    \n",
    "    bbs_all = np.insert(bbs_all,bbs_all.shape[1],scores_all,axis=1)\n",
    "    pointss_all = np.transpose(pointss_all)\n",
    "    \n",
    "    bbs = bbs_all.copy()\n",
    "    pointss = pointss_all.copy()\n",
    "    \n",
    "    if len(bbs_all) > 0:# if at least one face is detected\n",
    "        #process only one face (center ?)  \n",
    "        bb,points = one_face(frame, bbs, pointss)\n",
    "        \n",
    "        draw_landmarks(frame, bb, points)# draw land marks on face   \n",
    "        \n",
    "        cv2.putText(frame, \"Roll: {0:.2f} (-50 to +50)\".format(find_roll(points)), (10,90), font, font_size, red, 1)  \n",
    "        cv2.putText(frame, \"Yaw: {0:.2f} (-100 to +100)\".format(find_yaw(points)), (10,100), font, font_size, red, 1)\n",
    "        cv2.putText(frame, \"Pitch: {0:.2f} (0 to 4)\".format(find_pitch(points)), (10,110), font, font_size, red, 1)\n",
    "        #cv2.putText(frame, \"smiles: {}, neutrals: {}, idframes: {}\".format(Nsmiles, Nneutrals, Nframesperid), (10,460), font, font_size, blue, 1)\n",
    "        Xfrontal, Yfrontal = find_pose(points)\n",
    "        cv2.putText(frame, \"Xfrontal: {0:.2f}\".format(Xfrontal), (10,130), font, font_size, red, 1)\n",
    "        cv2.putText(frame, \"Yfrontal: {0:.2f}\".format(Yfrontal), (10,140), font, font_size, red, 1)\n",
    "        \n",
    "        smile_ratio = find_smile(points) \n",
    "        if smile_ratio > 0.9:\n",
    "            cv2.putText(frame, \"Expression: Smile\", (10,160), font, font_size, green, 1)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Expression: Neutral\", (10,160), font, font_size, green, 1)\n",
    "            \n",
    "    else:\n",
    "        cv2.putText(frame_show, 'no face', (10,logo_size+200), font, font_size, blue, 2)\n",
    "                \n",
    "    res_max[0]=total_size[0]#-show_size\n",
    "    res_max[1]=total_size[1]-2*logo_size\n",
    "    \n",
    "    res_resize[1]=res_max[1]\n",
    "    res_resize[0]=res_max[1]/res_crop[1]*res_crop[0]\n",
    "\n",
    "    if  res_resize[0]>res_max[0]:\n",
    "        res_resize[0]=res_max[0]\n",
    "        res_resize[1]=int(res_max[0]/res_crop[0]*res_crop[1]/2)*2\n",
    "\n",
    "    frame_resize = cv2.resize(frame,(res_resize[1],res_resize[0]), interpolation = cv2.INTER_LINEAR)    \n",
    "    space_vert=(total_size[1]-res_resize[1]) // 2 \n",
    "\n",
    "    frame_show[:frame_resize.shape[0],space_vert:-space_vert,:]=frame_resize \n",
    "    \n",
    "    cv2.putText(frame_show, 'q: quit', (10,50), font, font_size, blue, 2)    \n",
    "    cv2.imshow('Pose Detection - Retina Face',frame_show)    \n",
    "    \n",
    "    if video_save:\n",
    "        video_out.write(frame)        \n",
    "        \n",
    "    key_pressed = cv2.waitKey(1) & 0xFF\n",
    "    option=[]\n",
    "    options=['Quit']\n",
    "    if key_pressed == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "if video_save:\n",
    "    video_out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b31ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
